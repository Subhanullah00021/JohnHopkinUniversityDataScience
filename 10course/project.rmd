


## Raw data used

```{r summary, include = FALSE, warning = FALSE}

dfsum <- data.frame(
        File = c('Blogs', 'News', 'Twitter'),
        Size = c(200.4242, 196.2775, 159.3641),
        Lines = c(0.899288, 0.077258, 2.360148),
        Words = c(38.154238, 2.693883, 30.218166)
        )

colnames(dfsum) <- c('File', 'Size (Mb)', 'Lines (Millions)', 'Words (Millions)')
```

```{r dataset, echo = FALSE}
dfsum
```

## Cleaning and preparing

```{r onedata, include = FALSE}
dftext <- data.frame(
        File = 'Final text',
        Size.Mb = 78.52802,
        Lines = 0.667337,
        Words.Millions = 14.18622
        )
colnames(dftext) <- c('File', 'Size (Mb)', 'Lines (Millions)', 'Words (Millions)')
```

```{r presentdf, echo = FALSE}
dftext
```

Note that the algorithm uses 14 millions words, 667 thousand lines, as well as nearly 80 Mb, the small size is due to the cleaning and optimization of the data.

## Katz back-off model

The model uses a Katz back-off model, which employs various list of n-grams, this includes a 5-gram, 4-gram, 3-gram, 2-gram and monogram. And predicts the next word based on the last words following the order previously presented.

```{r trigram, include = FALSE}
bigram <- data.frame(ngrams = c('i am', 'of the', 'in the', 'it is', 'i have'),
                     freq = c(54585, 51630, 49224, 48239, 28896),
                     prop = c(0.004036763, 0.003818230, 0.003640297,
                              0.003567453, 0.002136966))
```

```{r pressure, echo = FALSE}
head(bigram)
```

## Demo of the application

```{r demo, eval = FALSE}
for(i in 1:7){
           text <- paste(text, nextword(text))
}
```

```{r print, echo = FALSE}
text1 <- 'Initial text: I need to'
text2 <- 'Final text: I need to get my followers up in the morning'

writeLines(c(text1, text2))
```
